{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33624bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.014630Z",
     "start_time": "2024-11-25T08:11:08.084656900Z"
    }
   },
   "outputs": [],
   "source": [
    "import chess \n",
    "import chess.pgn\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import re \n",
    "\n",
    "# list each folder of folder \"games\"\n",
    "# get all folders\n",
    "folders = glob.glob(\"games-repro/*\")\n",
    "\n",
    "# sort folders by date\n",
    "folders.sort(key=os.path.getmtime)\n",
    "\n",
    "df = pd.DataFrame(columns=[\"white\", \"black\", \"white_elo\", \"black_elo\", \"result\", \"nmoves\", \"nstarting_move\", \"pgn_base\", \"temperature\", \"random_engine\", \"has_illegal\", \"illegal_move\", \"folder_name\"])\n",
    "\n",
    "pgn_base_tab=[]\n",
    "\n",
    "def pgn_base_encode(txt):\n",
    "    if not (txt in pgn_base_tab):\n",
    "        pgn_base_tab.append(txt)\n",
    "    return pgn_base_tab.index(txt)\n",
    "\n",
    "for folder in folders:\n",
    "\n",
    "    # for each folder:\n",
    "    # read PGN file \"game.pgn\"\n",
    "\n",
    "    # check that \"game.pgn\" exists\n",
    "    if not os.path.exists(folder + \"/game.pgn\"):\n",
    "        print(\"No game.pgn in \" + folder) # TODO\n",
    "        continue\n",
    "    \n",
    "    with open(folder + \"/game.pgn\") as pgn:\n",
    "        game = chess.pgn.read_game(pgn)\n",
    "\n",
    "    # read metainformation.txt\n",
    "    # get the GPT model and the engine (SF or random)\n",
    "    nmove_value = None\n",
    "    with open(folder + \"/metainformation.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    # Iterate over each line in the file\n",
    "    for line in lines:\n",
    "        # Check if the line contains the string 'nmove:'\n",
    "        if 'nmove:' in line:\n",
    "            # Split the line at 'nmove:' and take the second part\n",
    "            # Then strip leading and trailing whitespaces and convert to integer\n",
    "            nmove_value = int(line.split('nmove:')[1].strip())\n",
    "            # Print the extracted value\n",
    "            break\n",
    "    \n",
    "    if nmove_value is None:\n",
    "        nmove_value = 1 # default value\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    # number of moves\n",
    "    game_length = len(list(game.mainline_moves()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for each PGN:\n",
    "    # get the GPT model and the engine (SF or random)\n",
    "    white = game.headers[\"White\"]\n",
    "    black = game.headers[\"Black\"]\n",
    "\n",
    "\n",
    "    # get the Elo of the engine (if any)\n",
    "\n",
    "    # get the Elo of the player (if any)\n",
    "    white_elo = game.headers[\"WhiteElo\"]\n",
    "    black_elo = game.headers[\"BlackElo\"]\n",
    "\n",
    "    # get the result (or infer based on checkmates) # special case: no mate, or unifinished game due to wrong move\n",
    "    result = game.headers[\"Result\"]\n",
    "\n",
    "    has_illegal = False\n",
    "    illegal_move = ''\n",
    "    # check that UnknownSAN key is in game\n",
    "    # if not, continue\n",
    "    if 'UnknownSAN' in game.headers:\n",
    "        has_illegal = True\n",
    "        illegal_move = game.headers[\"UnknownSAN\"]\n",
    "        #print(\"warning: UnknownSAN in game\")\n",
    "        # continue\n",
    "\n",
    "    with open(folder + \"/metainformation.txt\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # find the content between 'temperature:' and ends of line\n",
    "    match_random = re.search(r'random_engine:([\\s\\S]+?)(\\n)', content, re.MULTILINE)\n",
    "    random_engine = None\n",
    "    if match_random:\n",
    "        random_engine = match_random.group(1).strip()\n",
    "        if 'True' in random_engine:\n",
    "            random_engine = True\n",
    "        elif 'False' in random_engine:\n",
    "            random_engine = False\n",
    "        else:   \n",
    "            print(\"random engine value unclear/unknwon\")\n",
    "    else:\n",
    "        random_engine = False\n",
    "        # print('No random engine found') # default value: False (note: should not happen)\n",
    "\n",
    "    with open(folder + \"/metainformation.txt\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # find the content between 'temperature:' and ends of line\n",
    "    match_temperature = re.search(r'temperature:([\\s\\S]+?)(\\n)', content, re.MULTILINE)\n",
    "    temperature = None\n",
    "    if match_temperature:\n",
    "        temperature = match_temperature.group(1).strip()\n",
    "        # print(extracted_content)\n",
    "    else:\n",
    "        temperature = 0.0\n",
    "        # print('No temperature found') # default value: 0\n",
    "\n",
    "    with open(folder + \"/metainformation.txt\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Find the content between 'base_pgn:' and another term at the start of a new line followed by ':'\n",
    "    match = re.search(r'base_pgn:([\\s\\S]+?)(^\\w+:)', content, re.MULTILINE)\n",
    "    pgn_base = ''\n",
    "    if match:\n",
    "        # Extract and clean up the matched content\n",
    "        pgn_base = match.group(1).strip()\n",
    "        # print(extracted_content)\n",
    "    else:\n",
    "        print('No base pgn found')\n",
    "\n",
    "    # put in dfframe\n",
    "    # columns: white, black, white_elo, black_elo, result\n",
    "    # append to dfframe\n",
    "\n",
    "    df = pd.concat([df,pd.DataFrame([[white, black, white_elo, black_elo, result, game_length, nmove_value, pgn_base_encode(pgn_base), temperature, random_engine, has_illegal, illegal_move, folder]],columns=[\"white\", \"black\", \"white_elo\", \"black_elo\", \"result\", \"nmoves\", \"nstarting_move\", \"pgn_base\", \"temperature\", \"random_engine\", \"has_illegal\", \"illegal_move\", \"folder_name\"])], ignore_index=True)\n",
    "\n",
    "\n",
    "    # compute stats\n",
    "    # scores in general, per Elo and chess engine, per GPT model\n",
    "    # ability to finish a game (with weaker models)\n",
    "\n",
    "    # first: Elo = 1700, GPT=3.5 instruct\n",
    "    # second: Elo = 1800, GPT=3.5 instruct\n",
    "    # ...\n",
    "\n",
    "print(pgn_base_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"temperature\"]==\"0.9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b28bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.124966600Z",
     "start_time": "2024-11-25T08:11:25.028869600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df), \"games compiled in the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e66f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.293460200Z",
     "start_time": "2024-11-25T08:11:25.042208800Z"
    }
   },
   "outputs": [],
   "source": [
    "import io \n",
    "\n",
    "\n",
    "# for column \"pgn_base\", I'd like to transform the string into a list of moves\n",
    "\n",
    "def pgn_to_list(pgn):\n",
    "    g = chess.pgn.read_game(io.StringIO(pgn))\n",
    "    g.mainline_moves()\n",
    "    return str(g.mainline_moves())\n",
    "\n",
    "def is_base_prompt(pgn):\n",
    "    return len(pgn_base_tab)==1\n",
    "\n",
    "def has_illegal_moves(pgn):\n",
    "    # exist g.headers[\"UnknownSAN\"] \n",
    "    g = chess.pgn.read_game(io.StringIO(pgn))\n",
    "    # key in array\n",
    "    return \"UnknownSAN\" in g.headers\n",
    "\n",
    "print(len(pgn_base_tab))\n",
    "\n",
    "df[\"base_pgn_prompt\"] = df[\"pgn_base\"].apply(is_base_prompt) # extract only prompt\n",
    "# df.sort_values(by=['nstarting_move'])\n",
    "df['temperature'] = pd.to_numeric(df['temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118f825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.400863300Z",
     "start_time": "2024-11-25T08:11:25.297618200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Frequencies based on piece colors\n",
    "players = ['gpt-3.5-turbo-instruct']\n",
    "color_breakdown = {}\n",
    "for player in players:\n",
    "    white_count = df[df['white'] == player].shape[0]\n",
    "    black_count = df[df['black'] == player].shape[0]\n",
    "    color_breakdown[player] = {'white': white_count, 'black': black_count}\n",
    "\n",
    "for player in players:\n",
    "    nwhite = color_breakdown[player]['white']\n",
    "    nblack = color_breakdown[player]['black']\n",
    "    print(\" *\", player, \":\", nwhite+nblack, \"games, among\", nwhite, \"with white piece and\", nblack, \"with black pieces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c098cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.400863300Z",
     "start_time": "2024-11-25T08:11:25.327332500Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13b053b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.438199Z",
     "start_time": "2024-11-25T08:11:25.390588400Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"games_db_repro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c35c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.522087900Z",
     "start_time": "2024-11-25T08:11:25.427100600Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score(df, model_name='gpt-3.5-turbo-instruct', percentage=True):\n",
    "    # Count the number of wins, losses, and draws for gpt-3.5-turbo-instruct\n",
    "    wins_as_white = df[(df['white'] == model_name) & (df['result'] == '1-0')].shape[0]\n",
    "    wins_as_black = df[(df['black'] == model_name) & (df['result'] == '0-1')].shape[0]\n",
    "    losses_as_white = df[(df['white'] == model_name) & (df['result'] == '0-1')].shape[0]\n",
    "    losses_as_black = df[(df['black'] == model_name) & (df['result'] == '1-0')].shape[0]\n",
    "    draws_as_white = df[(df['white'] == model_name) & (df['result'] == '1/2-1/2')].shape[0]\n",
    "    draws_as_black = df[(df['black'] == model_name) & (df['result'] == '1/2-1/2')].shape[0]\n",
    "\n",
    "    # Calculate total wins, losses, and draws\n",
    "    total_wins = wins_as_white + wins_as_black\n",
    "    total_losses = losses_as_white + losses_as_black\n",
    "    total_draws = draws_as_white + draws_as_black\n",
    "\n",
    "    if percentage:\n",
    "        return (total_wins + (total_draws * 0.5)) / (total_wins + total_losses + total_draws)\n",
    "    else:\n",
    "        return (total_wins + (total_draws * 0.5), total_wins + total_losses + total_draws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2edd56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:25.655816400Z",
     "start_time": "2024-11-25T08:11:25.517984300Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['white_elo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60387868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eee201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:26.447925900Z",
     "start_time": "2024-11-25T08:11:26.045572900Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_model_performance(df_m, model_gpt_name):    \n",
    "\n",
    "    print(f\"Analysis against SF (no random engine, no random first moves) for model: {model_gpt_name}\")\n",
    "\n",
    "    model_games = df_m.query(f\"(white == '{model_gpt_name}' or black == '{model_gpt_name}') and has_illegal == False\")\n",
    "    score_model = compute_score(model_games, model_name=model_gpt_name)\n",
    "    print(\"Score\", round(100*score_model, 2), \"% for games with only legal moves\")\n",
    "    score, t = compute_score(model_games, model_name=model_gpt_name, percentage=False)\n",
    "    print(\"Score\", score, \"for games with only legal moves (\", t, \"games)\")\n",
    "\n",
    "    tot = len(df_m.query(f\"(white == '{model_gpt_name}' or black == '{model_gpt_name}')\"))\n",
    "    print(\"Score\", round(100*(score/tot), 2), \"% for all games, being legal or illegal\")\n",
    "    print(\"Score\", score, \"for all games (\", tot, \"games)\")\n",
    "\n",
    "    vals_model = df_m.query(f\"(white == '{model_gpt_name}' or black == '{model_gpt_name}')\")['has_illegal'].value_counts()\n",
    "    ntot = vals_model.get(0, 0) + vals_model.get(1, 0)\n",
    "\n",
    "    print(\"Out of\", ntot ,\"games against SF,\", vals_model.get(0, 0), \"were legal games and\", vals_model.get(1, 0), \"were illegal games, hence\", round((vals_model.get(1, 0)/ntot)*100), \"% of illegal games.\")\n",
    "    print(vals_model.get(0, 0), \"legal games and\", vals_model.get(1, 0), \"illegal games\", \"(out of\", ntot, \"total games)\")\n",
    "    print(round((vals_model.get(1, 0)/ntot)*100), \"% of illegal games\")\n",
    "\n",
    "    print(\"Illegal moves are:\")\n",
    "    print(df_m.query(f\"(white == '{model_gpt_name}' or black == '{model_gpt_name}') and has_illegal == True\")['illegal_move'].value_counts().to_markdown())\n",
    "\n",
    "# Example usage:\n",
    "analyze_model_performance(df, 'gpt-3.5-turbo-instruct')\n",
    "# analyze_model_performance(df_non_random, 'gpt-4')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3ffaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:26.548835400Z",
     "start_time": "2024-11-25T08:11:26.214520600Z"
    }
   },
   "outputs": [],
   "source": [
    "illegal_gpt35vsSF_instruct = df[df[\"has_illegal\"]].copy()\n",
    "\n",
    "print(illegal_gpt35vsSF_instruct[[\"temperature\"]].value_counts())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036daf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    print(\"{}% des parties sont jouées avec une température de {}\".format(round(len(df.query(\"`temperature` == {}\".format(str(t))))/len(df)*100,2),t))\n",
    "    print(\"{}% des parties jouées avec une température de {} produisent des moves illégaux\".format(round(len(illegal_gpt35vsSF_instruct.query(\"`temperature` == {}\".format(str(t))))/len(df.query(\"`temperature` == {}\".format(str(t))))*100,2),t))\n",
    "    print(\"{}% des parties jouées avec une température de {} produisent des moves vraiment illégaux\\n\".format(round(len(illegal_gpt35vsSF_instruct.query(\"`temperature` == {} and illegal_move != '1-0'\".format(str(t))))/len(df.query(\"`temperature` == {}\".format(str(t))))*100,2),t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11222b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    print(\"illegal moves for temperature {}\".format(t))\n",
    "    print(illegal_gpt35vsSF_instruct.query(\"`temperature` == {} and illegal_move != '1-0'\".format(str(t)))['illegal_move'].value_counts().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5d116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:11:27.348034600Z",
     "start_time": "2024-11-25T08:11:26.329391600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Qualitative analysis of illegal moves:\")\n",
    "\n",
    "# A list to store your data\n",
    "data_list = []\n",
    "\n",
    "illegal_moves10 = df.query(\"(white == 'gpt-3.5-turbo-instruct' or black == 'gpt-3.5-turbo-instruct') and illegal_move == '1-0'\")\n",
    "\n",
    "for index, row in illegal_moves10.iterrows():\n",
    "    if row['white'] == 'gpt-3.5-turbo-instruct':\n",
    "        color = \"White\"\n",
    "    else:\n",
    "        color = \"Black\"\n",
    "\n",
    "    # read the PGN file\n",
    "    with open(row['folder_name'] + \"/game.pgn\") as pgn:\n",
    "        game = chess.pgn.read_game(pgn)\n",
    "        board = game.board()\n",
    "        for move in game.mainline_moves():\n",
    "            board.push(move)\n",
    "\n",
    "        stockfish = Stockfish(\"./stockfish/stockfish-ubuntu-x86-64-avx2\")\n",
    "        stockfish.set_position([str(m) for m in game.mainline_moves()])\n",
    "        # stockfish._go_time(5000)\n",
    "        ev = stockfish.get_evaluation()\n",
    "\n",
    "        # Create an evaluation string\n",
    "        if ev['type'] == 'cp':\n",
    "            evaluation = str(ev['value']/100)\n",
    "        else:\n",
    "            evaluation = \"Mate in \" + str(ev['value'])\n",
    "\n",
    "        # Append the dictionary to your list\n",
    "        data_list.append({\n",
    "            \"GPT Color\": color,\n",
    "            \"Assessment\": evaluation\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a dataframe\n",
    "df_results = pd.DataFrame(data_list)\n",
    "\n",
    "# If you want to see the first few rows of your dataframe:\n",
    "print(df_results.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4c710",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-25T08:11:27.238982800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_games(df_l, model_name):\n",
    "    # Query for games played by the given model\n",
    "    model_games = df_l.query(f\"(white == '{model_name}' or black == '{model_name}')\")\n",
    "    model_games['nmoves'] = model_games['nmoves'] / 2\n",
    "    \n",
    "    # Sum of moves played by the model\n",
    "    white_nmoves = df_l.query(f\"white == '{model_name}'\")['nmoves'].sum() / 2\n",
    "    black_nmoves = df_l.query(f\"black == '{model_name}'\")['nmoves'].sum() / 2\n",
    "    t_moves = model_games['nmoves'].sum()\n",
    "\n",
    "    temp_tab = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "    # Count of illegal moves\n",
    "    illegal_moves = len(df_l.query(f\"(white == '{model_name}' or black == '{model_name}') and has_illegal == True\"))\n",
    "    illegal_moves_10 = len(df_l.query(f\"(white == '{model_name}' or black == '{model_name}') and has_illegal == True and illegal_move != '1-0'\"))\n",
    "\n",
    "    df_t = [model_games.query(\"has_illegal==False and `temperature`=={}\".format(round(t,1))) for t in temp_tab ]\n",
    "\n",
    "    # Printing results\n",
    "    print(\"illegal moves\", round(illegal_moves/t_moves, 5)*100, \"%\")\n",
    "    print(\"illegal_moves without 1-0:\", round(illegal_moves_10/t_moves, 5)*100, \"%\")\n",
    "    print(\"Total number of moves played by\", model_name, \":\", t_moves)\n",
    "    print(\"White played\", white_nmoves, \"moves\")\n",
    "    print(\"Black played\", black_nmoves, \"moves\")\n",
    "    print(\"Number of moves against SF\")\n",
    "    print(\"The longest game was\", model_games['nmoves'].max(), \"moves\")\n",
    "    print(\"The shortest game was\", model_games['nmoves'].min(), \"moves\")\n",
    "    print(\"The average game length was\", model_games['nmoves'].mean(), \"moves\")\n",
    "    print(\"The median game length was\", model_games['nmoves'].median(), \"moves\")\n",
    "    x =[]\n",
    "    y =[]\n",
    "    for i in range(11) :\n",
    "        x.append( temp_tab[i])\n",
    "        y.append(df_t[i]['nmoves'].mean())\n",
    "        print(\"The mean game length for temperature {} is {} moves\".format(x[i], y[i]))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting box plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(model_games['nmoves'])\n",
    "    plt.title(f\"Number of moves per game against SF\")\n",
    "    plt.ylabel(\"Number of moves\")\n",
    "    plt.xlabel(f\"{model_name}\")\n",
    "    plt.savefig(f\"{model_name}_games_nmoves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "analyze_games(df, 'gpt-3.5-turbo-instruct')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1215102",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-25T08:11:27.288702200Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_average(df, model_name='gpt-3.5-turbo-instruct'):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert Elo columns to numeric, setting errors='coerce' to handle non-numeric values\n",
    "    df_copy['white_elo'] = pd.to_numeric(df_copy['white_elo'], errors='coerce')\n",
    "    df_copy['black_elo'] = pd.to_numeric(df_copy['black_elo'], errors='coerce')\n",
    "\n",
    "    # Filter out rows where gpt-3.5-turbo-instruct is the player and get the opponent's Elo\n",
    "    opponent_elo_white = df_copy[df_copy['white'] == model_name]['black_elo']\n",
    "    opponent_elo_black = df_copy[df_copy['black'] == model_name]['white_elo']\n",
    "\n",
    "    # Concatenate the Elo ratings of opponents when gpt-3.5-turbo-instruct played as white and black\n",
    "    all_opponent_elo = pd.concat([opponent_elo_white, opponent_elo_black])\n",
    "\n",
    "    # Calculate the average Elo rating of the opponents, excluding missing or NaN values\n",
    "    average_opponent_elo = all_opponent_elo.mean()\n",
    "\n",
    "    return average_opponent_elo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff0c1e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-25T08:11:27.290364100Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "\n",
    "def computation_Elo(df,  initial_guess = 1700, model_name='gpt-3.5-turbo-instruct', K=400):\n",
    "    # Given values\n",
    "    S_A = compute_score(df, model_name)  # The score of Player A\n",
    "    R_B = compute_average(df, model_name)  # The rating of Player \n",
    "\n",
    "    # Define the equation to solve for R_A\n",
    "    def equation(R_A, *data):\n",
    "        S_A, R_B = data\n",
    "        return S_A - 1 / (1 + 10**((R_B - R_A) / K)) \n",
    "\n",
    "    # Solve the equation for R_A\n",
    "    R_A_solution = fsolve(equation, initial_guess, args=(S_A, R_B))\n",
    "\n",
    "    # Extract the calculated R_A value\n",
    "    R_A_calculated = float(R_A_solution[0])\n",
    "    return R_A_calculated\n",
    "y1 = []\n",
    "print(\"global elo : {}\".format(computation_Elo(df)))\n",
    "for t in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    elo = computation_Elo(df.query(\"temperature == \"+str(t)))\n",
    "    print(\"t={} elo : {}\".format(t,elo))\n",
    "    y1.append(elo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec0f29",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-25T08:11:27.290364100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y2WithIllegal = []\n",
    "y2WithoutIllegal = []\n",
    "def lookup_fide_table(score):\n",
    "        # Data here: https://handbook.fide.com/chapter/B022017\n",
    "        data = {\n",
    "            'p': [1.0, .99, .98, .97, .96, .95, .94, .93, .92, .91, .90, .89, .88, .87, .86, .85, .84,\n",
    "                .83, .82, .81, .80, .79, .78, .77, .76, .75, .74, .73, .72, .71, .70, .69, .68, .67,\n",
    "                .66, .65, .64, .63, .62, .61, .60, .59, .58, .57, .56, .55, .54, .53, .52, .51, .50,\n",
    "                .49, .48, .47, .46, .45, .44, .43, .42, .41, .40, .39, .38, .37, .36, .35, .34, .33,\n",
    "                .32, .31, .30, .29, .28, .27, .26, .25, .24, .23, .22, .21, .20, .19, .18, .17, .16,\n",
    "                .15, .14, .13, .12, .11, .10, .09, .08, .07, .06, .05, .04, .03, .02, .01],\n",
    "            'dp': [800, 677, 589, 538, 501, 470, 444, 422, 401, 383, 366, 351, 336, 322, 309, 296, 284,\n",
    "                273, 262, 251, 240, 230, 220, 211, 202, 193, 184, 175, 166, 158, 149, 141, 133, 125,\n",
    "                117, 110, 102, 95, 87, 80, 72, 65, 57, 50, 43, 36, 29, 21, 14, 7, 0, -7, -14, -21, \n",
    "                -29, -36, -43, -50, -57, -65, -72, -80, -87, -95, -102, -110, -117, -125, -133, -141,\n",
    "                -149, -158, -166, -175, -184, -193, -202, -211, -220, -230, -240, -251, -262, -273, \n",
    "                -284, -296, -309, -322, -336, -351, -366, -383, -401, -444, -470, -501, -538, -589, -677, -800]\n",
    "        }\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_fide = pd.DataFrame(data)\n",
    "\n",
    "        rscore = round(score, 2)\n",
    "\n",
    "        # lookup score in table\n",
    "        dp = df_fide.loc[df_fide['p'] == rscore, 'dp'].iloc[0]\n",
    "        return dp\n",
    "\n",
    "def fide_elo_computation(dfe, model_name, score=None):\n",
    "    average_opponents_ratings = compute_average(dfe, model_name)\n",
    "    if score is None:\n",
    "        score = compute_score(dfe, model_name)\n",
    "    dp = lookup_fide_table(score)\n",
    "\n",
    "\n",
    "    return average_opponents_ratings + dp\n",
    "\n",
    "                    \n",
    "def compute_elo(df_l, model_gpt_name):\n",
    "    df_elo = df_l.query(f\"(white == '{model_gpt_name}' or black == '{model_gpt_name}') and `has_illegal` == False\")\n",
    "    fide_elo_value = round(fide_elo_computation(df_elo, model_gpt_name), 0)\n",
    "    y2WithoutIllegal.append(fide_elo_value)\n",
    "    print(f\"{fide_elo_value} Elo for {model_gpt_name} against SF and only with legal games/moves\")\n",
    "    \n",
    "    df_elo_withillegal = df_l.query(f\"(white == '{model_gpt_name}' or black == '{model_gpt_name}')\")\n",
    "    s, t = compute_score(df_elo_withillegal, model_gpt_name, percentage=False)\n",
    "    sc = s / len(df_elo_withillegal)\n",
    "    fide_elo_with_illegal_value = round(fide_elo_computation(df_elo_withillegal, model_gpt_name, sc), 0)\n",
    "    y2WithIllegal.append(fide_elo_with_illegal_value)\n",
    "    print(f\"{fide_elo_with_illegal_value} Elo for {model_gpt_name} against SF and with all games\")\n",
    "\n",
    "# Usage:\n",
    "for t in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    print(\"temperature == \"+str(t))\n",
    "    compute_elo(df[df[\"temperature\"] == t], 'gpt-3.5-turbo-instruct')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2WithIllegal)\n",
    "plt.plot(x, y2WithoutIllegal)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
